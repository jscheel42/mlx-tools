diff --git a/mlx_lm/server.py b/mlx_lm/server.py
index 425cd38..b8305a6 100644
--- a/mlx_lm/server.py
+++ b/mlx_lm/server.py
@@ -172,7 +172,6 @@ def process_message_content(messages):
 
 
 class LRUPromptCache:
-
     @dataclass
     class CacheEntry:
         prompt_cache: List[Any]
@@ -938,9 +937,9 @@ class APIHandler(BaseHTTPRequestHandler):
 
         indent = "\t"  # Backslashes can't be inside of f-strings
         logging.debug(f"Incoming Request Body: {json.dumps(self.body, indent=indent)}")
-        assert isinstance(
-            self.body, dict
-        ), f"Request should be dict, but got {type(self.body)}"
+        assert isinstance(self.body, dict), (
+            f"Request should be dict, but got {type(self.body)}"
+        )
 
         # Extract request parameters from the body
         self.stream = self.body.get("stream", False)
@@ -1527,7 +1526,10 @@ def run(
     handler_class=APIHandler,
 ):
     server_address = (host, port)
-    response_generator = ResponseGenerator(model_provider, LRUPromptCache())
+    cache_size = getattr(model_provider.cli_args, "prompt_cache_size", 10)
+    response_generator = ResponseGenerator(
+        model_provider, LRUPromptCache(max_size=cache_size)
+    )
     infos = socket.getaddrinfo(
         *server_address, type=socket.SOCK_STREAM, flags=socket.AI_PASSIVE
     )
@@ -1645,6 +1647,12 @@ def main():
         help="""A JSON formatted string of arguments for the tokenizer's apply_chat_template, e.g. '{"enable_thinking":false}'""",
         default="{}",
     )
+    parser.add_argument(
+        "--prompt-cache-size",
+        type=int,
+        default=10,
+        help="Maximum size of the LRU prompt cache (default: 10)",
+    )
     args = parser.parse_args()
     if mx.metal.is_available():
         wired_limit = mx.metal.device_info()["max_recommended_working_set_size"]
