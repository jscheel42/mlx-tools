# NOTE: Includes a guard against empty batch KV cache filtering to avoid
# ValueError when all active batch entries finish simultaneously.
diff --git a/mlx_lm/server.py b/mlx_lm/server.py
index 425cd38..f376045 100644
--- a/mlx_lm/server.py
+++ b/mlx_lm/server.py
@@ -172,7 +172,6 @@ def process_message_content(messages):
 
 
 class LRUPromptCache:
-
     @dataclass
     class CacheEntry:
         prompt_cache: List[Any]
@@ -792,6 +791,20 @@ class ResponseGenerator:
                     cache += make_prompt_cache(self.model_provider.draft_model)
 
             # Process the prompt and generate tokens
+            # Add KV cache quantization parameters if specified
+            kv_kwargs = {}
+            if (
+                hasattr(self.model_provider.cli_args, "kv_bits")
+                and self.model_provider.cli_args.kv_bits
+            ):
+                kv_kwargs["kv_bits"] = self.model_provider.cli_args.kv_bits
+                kv_kwargs["kv_group_size"] = (
+                    self.model_provider.cli_args.kv_group_size
+                )
+                kv_kwargs["quantized_kv_start"] = (
+                    self.model_provider.cli_args.quantized_kv_start
+                )
+
             for gen in stream_generate(
                 model=model,
                 tokenizer=tokenizer,
@@ -803,6 +816,7 @@ class ResponseGenerator:
                 draft_model=draft_model,
                 num_draft_tokens=args.num_draft_tokens,
                 prompt_progress_callback=progress,
+                **kv_kwargs,
             ):
                 top_tokens = None
                 if args.logprobs > 0:
@@ -938,14 +952,28 @@ class APIHandler(BaseHTTPRequestHandler):
 
         indent = "\t"  # Backslashes can't be inside of f-strings
         logging.debug(f"Incoming Request Body: {json.dumps(self.body, indent=indent)}")
-        assert isinstance(
-            self.body, dict
-        ), f"Request should be dict, but got {type(self.body)}"
+        assert isinstance(self.body, dict), (
+            f"Request should be dict, but got {type(self.body)}"
+        )
 
         # Extract request parameters from the body
         self.stream = self.body.get("stream", False)
         self.stream_options = self.body.get("stream_options", None)
-        self.requested_model = self.body.get("model", "default_model")
+
+        # Map custom model ID back to actual model path
+        requested_model = self.body.get("model", "default_model")
+        custom_model_id = getattr(self.response_generator.cli_args, "model_id", None)
+        if custom_model_id and requested_model == custom_model_id:
+            # Client requested the custom model ID, use the actual model path for loading
+            self.requested_model = (
+                self.response_generator.cli_args.model or "default_model"
+            )
+            # But keep the custom ID for the response
+            self.response_model_id = custom_model_id
+        else:
+            self.requested_model = requested_model
+            self.response_model_id = requested_model
+
         self.requested_draft_model = self.body.get("draft_model", "default_model")
         self.num_draft_tokens = self.body.get(
             "num_draft_tokens", self.response_generator.cli_args.num_draft_tokens
@@ -1088,10 +1116,10 @@ class APIHandler(BaseHTTPRequestHandler):
 
         # Static response
         response = {
-            "id": self.request_id,
-            "system_fingerprint": self.system_fingerprint,
-            "object": self.object_type,
-            "model": self.requested_model,
+            "id": "chatcmpl-" + str(uuid.uuid4()),
+            "system_fingerprint": get_system_fingerprint(),
+            "object": "chat.completion",
+            "model": self.response_model_id,
             "created": self.created,
             "choices": [
                 {
@@ -1385,7 +1413,7 @@ class APIHandler(BaseHTTPRequestHandler):
             "id": self.request_id,
             "system_fingerprint": self.system_fingerprint,
             "object": "chat.completion",
-            "model": self.requested_model,
+            "model": self.response_model_id,
             "created": self.created,
             "choices": [],
             "usage": {
@@ -1484,33 +1512,47 @@ class APIHandler(BaseHTTPRequestHandler):
             file_names = {f.file_path.name for f in repo.refs["main"].files}
             return all(f in file_names for f in files)
 
-        # Scan the cache directory for downloaded mlx models
-        hf_cache_info = scan_cache_dir()
-        downloaded_models = [
-            repo for repo in hf_cache_info.repos if probably_mlx_lm(repo)
-        ]
-
-        # Create a list of available models
-        models = [
-            {
-                "id": repo.repo_id,
-                "object": "model",
-                "created": self.created,
-            }
-            for repo in downloaded_models
-        ]
-
-        if self.response_generator.cli_args.model:
-            model_path = Path(self.response_generator.cli_args.model)
-            if model_path.exists():
-                model_id = str(model_path.resolve())
-                models.append(
-                    {
-                        "id": model_id,
-                        "object": "model",
-                        "created": self.created,
-                    }
-                )
+        # If model-id is specified, only list the specified model (single-model mode)
+        # Otherwise, scan cache for all available models (multi-model mode)
+        custom_model_id = getattr(self.response_generator.cli_args, "model_id", None)
+
+        if custom_model_id and self.response_generator.cli_args.model:
+            # Single-model mode: only list the specified model with custom ID
+            models = [
+                {
+                    "id": custom_model_id,
+                    "object": "model",
+                    "created": self.created,
+                }
+            ]
+        else:
+            # Multi-model mode: scan cache and list all available models
+            hf_cache_info = scan_cache_dir()
+            downloaded_models = [
+                repo for repo in hf_cache_info.repos if probably_mlx_lm(repo)
+            ]
+
+            models = [
+                {
+                    "id": repo.repo_id,
+                    "object": "model",
+                    "created": self.created,
+                }
+                for repo in downloaded_models
+            ]
+
+            # Add the specified model if provided
+            if self.response_generator.cli_args.model:
+                model_path = Path(self.response_generator.cli_args.model)
+                if model_path.exists():
+                    model_id = str(model_path.resolve())
+                    models.append(
+                        {
+                            "id": model_id,
+                            "object": "model",
+                            "created": self.created,
+                        }
+                    )
 
         response = {"object": "list", "data": models}
 
@@ -1527,7 +1569,10 @@ def run(
     handler_class=APIHandler,
 ):
     server_address = (host, port)
-    response_generator = ResponseGenerator(model_provider, LRUPromptCache())
+    cache_size = getattr(model_provider.cli_args, "prompt_cache_size", 10)
+    response_generator = ResponseGenerator(
+        model_provider, LRUPromptCache(max_size=cache_size)
+    )
     infos = socket.getaddrinfo(
         *server_address, type=socket.SOCK_STREAM, flags=socket.AI_PASSIVE
     )
@@ -1556,6 +1601,11 @@ def main():
         type=str,
         help="The path to the MLX model weights, tokenizer, and config",
     )
+    parser.add_argument(
+        "--model-id",
+        type=str,
+        help="Override the model ID returned by the API (default: full model path)",
+    )
     parser.add_argument(
         "--adapter-path",
         type=str,
@@ -1645,6 +1695,30 @@ def main():
         help="""A JSON formatted string of arguments for the tokenizer's apply_chat_template, e.g. '{"enable_thinking":false}'""",
         default="{}",
     )
+    parser.add_argument(
+        "--prompt-cache-size",
+        type=int,
+        default=10,
+        help="Maximum size of the LRU prompt cache (default: 10)",
+    )
+    parser.add_argument(
+        "--kv-bits",
+        type=int,
+        default=None,
+        help="Number of bits for KV cache quantization (4 or 8 recommended, default: no quantization)",
+    )
+    parser.add_argument(
+        "--kv-group-size",
+        type=int,
+        default=64,
+        help="Group size for KV cache quantization (default: 64)",
+    )
+    parser.add_argument(
+        "--quantized-kv-start",
+        type=int,
+        default=0,
+        help="Step to start quantizing KV cache (default: 0, quantize from start)",
+    )
     args = parser.parse_args()
    if mx.metal.is_available():
        wired_limit = mx.metal.device_info()["max_recommended_working_set_size"]

diff --git a/mlx_lm/models/cache.py b/mlx_lm/models/cache.py
index 21d3e2c..b1b15db 100644
--- a/mlx_lm/models/cache.py
+++ b/mlx_lm/models/cache.py
@@ -907,10 +907,19 @@ class BatchKVCache(_BaseCache):
     def filter(self, batch_indices):
         """
         In-place filter to keep just the given indices in the cache.
         """
-        self.keys = self.keys[batch_indices]
-        self.values = self.values[batch_indices]
         self.offset = self.offset[batch_indices]
         self.left_padding = self.left_padding[batch_indices]
+        if self.keys is None or self.values is None:
+            return
+        if self.keys.size == 0 or self.values.size == 0:
+            self.keys = None
+            self.values = None
+            self._idx = 0
+            return
+        self.keys = self.keys[batch_indices]
+        self.values = self.values[batch_indices]
@@ -1211,10 +1220,20 @@ class BatchRotatingKVCache(_BaseCache):
     def filter(self, batch_indices):
         """
         In-place filter to keep just the given indices in the cache.
         """
-        self.keys = self.keys[batch_indices]
-        self.values = self.values[batch_indices]
         self.offset = self.offset[batch_indices]
         self.left_padding = self.left_padding[batch_indices]
+        if self.keys is None or self.values is None:
+            return
+        if self.keys.size == 0 or self.values.size == 0:
+            self.keys = None
+            self.values = None
+            self._idx = 0
+            self._offset = 0
+            self.rotated = False
+            return
+        self.keys = self.keys[batch_indices]
+        self.values = self.values[batch_indices]
